{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_CNN_Part_B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipnarayan501/CNN/blob/main/DL_CNN_Part_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART - B Fine-Tuning a Pre-Trained model**"
      ],
      "metadata": {
        "id": "wIYkswfOU1Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Google colab to drive "
      ],
      "metadata": {
        "id": "XOQFMPlwUUTV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_r21VxChtR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d11941-b54d-4cfd-8146-092430aa3c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Link to download dataset\n",
        "#!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
        "#Mounting to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import shutil and splitting the dataset "
      ],
      "metadata": {
        "id": "nTChDgQeVQ6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil  # provides the ability to copy and removal of files\n",
        "import os \n",
        "import random\n",
        "try:\n",
        "    shutil.rmtree(\"inaturalist_12K\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Unzip the natural datset     \n",
        "zip_path = \"drive/MyDrive/nature_12K.zip\"\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q nature_12K.zip\n",
        "\n",
        "try:\n",
        "    os.remove(\"nature_12K.zip\")\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    shutil.rmtree(\"inaturalist_12K/validation/\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# dataset directory \n",
        "temp = os.listdir(\"inaturalist_12K/train\")\n",
        "class_names = [name for name in temp if name != \".DS_Store\"] \n",
        "\n",
        "#Spliting valdiation dataset \n",
        "validation_split = 0.1 #splitting dataset 10%\n",
        "def spliting_train_data(validation_split = 0.1):\n",
        "  os.mkdir(\"inaturalist_12K/validation/\")\n",
        "  for each_class in class_names:\n",
        "      os.mkdir(\"inaturalist_12K/validation/\"+each_class)\n",
        "      train_images = os.listdir(\"inaturalist_12K/train/\"+each_class)\n",
        "      train_images_per_class = list(train_images)\n",
        "      #print(len(train_images_per_class))\n",
        "      random.shuffle(train_images_per_class) # Shuffle the training images\n",
        "\n",
        "      validation_images_per_class = train_images_per_class[:round(validation_split*len(train_images_per_class))]\n",
        "     # print(len(validation_images_per_class))\n",
        "\n",
        "      for im in validation_images_per_class:\n",
        "          shutil.move(\"inaturalist_12K/train/\"+each_class+\"/\"+im, \"inaturalist_12K/validation/\"+each_class+\"/\"+im)\n",
        "spliting_train_data(validation_split)\n"
      ],
      "metadata": {
        "id": "8-91XS9fi_e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring the Data"
      ],
      "metadata": {
        "id": "WQVhzYffXYya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = 0\n",
        "val = 0\n",
        "test = 0\n",
        "\n",
        "for each_class in class_names:\n",
        "      train_images = os.listdir(\"inaturalist_12K/train/\"+each_class)\n",
        "      train+=len(list(train_images))\n",
        "\n",
        "      validation_images = os.listdir(\"inaturalist_12K/validation/\"+each_class)\n",
        "      \n",
        "      val+=len(list(validation_images))\n",
        "\n",
        "      test_images = os.listdir(\"inaturalist_12K/val/\"+each_class)\n",
        "      \n",
        "      test+=len(list(test_images))\n",
        " # print training validation test      \n",
        "print(\"training examples\" , train)\n",
        "print(\"validation Examples = \", val)\n",
        "print(\"test examples = \", test)"
      ],
      "metadata": {
        "id": "kqYnlBbrAfHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf59bcd-2e7f-404f-9e19-c9d153a8543c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training 9000\n",
            "validation Examples =  1000\n",
            "test examples =  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required libriaries for pre trained module "
      ],
      "metadata": {
        "id": "O4nfbgpxXqfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import os  \n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout, BatchNormalization\n",
        "\n",
        "\n",
        "# implement weights and biases \n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from wandb.keras import WandbCallback\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "jfaDSZlTi_hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the Train Test and Validation generators\n"
      ],
      "metadata": {
        "id": "wRmD-pv_X2-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"inaturalist_12K\"\n",
        "#data_augmentation = True\n",
        "# data prepartion \n",
        "def data_preparation(data_dir , data_augmentation , batch_size):\n",
        "  train_dir = os.path.join(data_dir, \"train\")\n",
        "  val_dir = os.path.join(data_dir, \"validation\")\n",
        "  test_dir = os.path.join(data_dir, \"val\")\n",
        "  if data_augmentation == True:\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                      height_shift_range=0.2,\n",
        "                                      width_shift_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      zoom_range=0.2,\n",
        "                                      fill_mode=\"nearest\")\n",
        "  else:\n",
        "      train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          train_dir,\n",
        "          target_size=(224, 224),\n",
        "          shuffle=True,\n",
        "          color_mode=\"rgb\",\n",
        "          batch_size=batch_size,\n",
        "          class_mode='categorical')\n",
        "\n",
        "  val_generator = val_datagen.flow_from_directory(\n",
        "          val_dir,\n",
        "          target_size=(224, 224),\n",
        "          shuffle=True,\n",
        "          color_mode=\"rgb\",\n",
        "          batch_size=batch_size,\n",
        "          class_mode='categorical')\n",
        "  \n",
        "  test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size= (224, 224),\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "  \n",
        "  \n",
        "  return train_generator , val_generator, test_generator\n",
        "#train_generator , val_generator, test_generator = data_preparation(data_dir , data_augmentation = True , batch_size = 250)"
      ],
      "metadata": {
        "id": "P5t4ApQDi_kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def models(pretrained_model_name):\n",
        "      # shape image size \n",
        "    shape = (224 ,224, 3)\n",
        "\n",
        "    # adding a pretrained model without the top dense layer\n",
        "    if pretrained_model_name == 'ResNet50':\n",
        "      pretrained_model = tf.keras.applications.ResNet50(include_top=False,input_shape=shape, weights='imagenet')\n",
        "    elif pretrained_model_name == 'InceptionV3':\n",
        "      pretrained_model = tf.keras.applications.InceptionV3(include_top=False,input_shape= shape, weights='imagenet')    \n",
        "    elif pretrained_model_name == 'Xception':\n",
        "      pretrained_model = tf.keras.applications.Xception(include_top=False,input_shape= shape, weights='imagenet')\n",
        "    elif pretrained_model_name == 'InceptionResNetV2':\n",
        "      pretrained_model = tf.keras.applications.InceptionResNetV2(include_top=False,input_shape= shape, weights='imagenet')\n",
        "    \n",
        "    else:\n",
        "      raise Exception('no pretrained model given')\n",
        "    return pretrained_model\n"
      ],
      "metadata": {
        "id": "vfmGfrzH2kh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement pre trained InceptionV3, InceptionResNetV2, ResNet50, Xception"
      ],
      "metadata": {
        "id": "T0nTOiOVX-u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_model(pretrained_model_name, dropout, dense_layer, pre_layer_train=None):\n",
        "    \n",
        "  # keras sequential model based on a pre-trained model intended to be fine tuned\n",
        "  \n",
        "\n",
        "    # shape image size \n",
        "    shape = (224 ,224, 3)\n",
        "          \n",
        "    pretrained_model = models(pretrained_model_name)\n",
        "    #freeze all layers\n",
        "    for layer in pretrained_model.layers:\n",
        "        layer.trainable=False \n",
        "    \n",
        "    #setting  top layers as trainable\n",
        "    if pre_layer_train:\n",
        "      for layer in pretrained_model.layers[-pre_layer_train:]:\n",
        "        layer.trainable=True\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    #adding pretrained model\n",
        "    model.add(pretrained_model)\n",
        "    # flatten layer  \n",
        "    model.add(Flatten()) \n",
        "    #Adding a dense layer\n",
        "    model.add(Dense(dense_layer, activation= 'relu'))\n",
        "    #Adding dropout\n",
        "    model.add(Dropout(dropout))\n",
        "    #output layer with softmax  \n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        " \n",
        "    return model"
      ],
      "metadata": {
        "id": "0HWNp_Fzi_mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration wandb\n"
      ],
      "metadata": {
        "id": "A-Nqbud5YRoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Default values for hyper-parameters\n",
        "    config_defaults = {\n",
        "        \"data_augmentation\": True,\n",
        "        \"batch_size\": 250,\n",
        "        \"dropout\": 0.1,\n",
        "        \"dense_layer\": 256,\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"epochs\": 5,\n",
        "        \"pre_layer_train\": None,\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    # Local variables, values obtained from wandb config\n",
        "    data_augmentation = config.data_augmentation\n",
        "    batch_size = config.batch_size\n",
        "    dropout = config.dropout\n",
        "    dense_layer = config.dense_layer\n",
        "    learning_rate = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    pre_layer_train = config.pre_layer_train\n",
        "\n",
        "    \n",
        "   # Display the hyperparameters\n",
        "    run_name = \"pre_train_mdl_{}_aug_{}_bs_{}_ep_{}_dropout_{}_dense_{}\".format(pre_train_model, data_augmentation, batch_size,epochs, dropout, dense_layer)\n",
        "    print(run_name)\n",
        "\n",
        "    # Create the data generators\n",
        "    train_generator , val_generator, test_generator = data_preparation(data_dir , data_augmentation = data_augmentation, batch_size = batch_size)\n",
        "    \n",
        "    # Define the model\n",
        "    model = pretrain_model(pretrained_model_name = pre_train_model, dropout = dropout, dense_layer = dense_layer, pre_layer_train=pre_layer_train)\n",
        "    #print(model.count_params())\n",
        "\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate = learning_rate), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    # Early Stopping callback\n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "\n",
        "    # To save the model with best validation accuracy\n",
        "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "    #Training the model\n",
        "    history = model.fit(train_generator,\n",
        "                        steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
        "                        validation_data = val_generator,\n",
        "                        validation_steps = val_generator.n//val_generator.batch_size,\n",
        "                        epochs=epochs, verbose = 2, \n",
        "                        callbacks=[WandbCallback(), earlyStopping, checkpoint])\n",
        "      \n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    # Meaningful name for the run\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "    wandb.run.finish()\n",
        "    return history"
      ],
      "metadata": {
        "id": "dJuaKSnFD2oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sweeps configuration for hyper parameters "
      ],
      "metadata": {
        "id": "6beHG2WCZI56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"CNN_PartB\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"val_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"bayes\",\n",
        "  \"parameters\": {\n",
        "        \"data_augmentation\": {\n",
        "            \"values\": [True, False]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [128, 256]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001, 0.0001]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0, 0.1,0.2,0.4]\n",
        "        },\n",
        "        \"dense_layer\": {\n",
        "            \"values\": [128, 256, 512]\n",
        "        },\n",
        "          \"pre_layer_train\": {\n",
        "            \"values\": [None, 10, 20]\n",
        "        },\n",
        "\n",
        "      \n",
        "           }\n",
        "}\n",
        "\n",
        "\n",
        "# Generates a sweep id\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"moni6264\", project=\"test_cnn_partb\")\n"
      ],
      "metadata": {
        "id": "vjKhNLrmo2qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running wandb"
      ],
      "metadata": {
        "id": "mxrc4hXLKvwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note:  You might get different result because of shuffling of dataset\n",
        "pre_train_model = \"InceptionResNetV2\"   #you can change model for wandb sweeps\n",
        "sweep_id = \"y68teyjh\"\n",
        "wandb.agent(sweep_id, train,entity=\"moni6264\", project=\"test_cnn_partb\", count=10)"
      ],
      "metadata": {
        "id": "Za5GdVu6iCEf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}