{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_CNN_Part_B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipnarayan501/CNN/blob/main/DL_CNN_Part_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART - B Fine-Tuning a Pre-Trained model**"
      ],
      "metadata": {
        "id": "wIYkswfOU1Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Google colab to drive "
      ],
      "metadata": {
        "id": "XOQFMPlwUUTV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_r21VxChtR5",
        "outputId": "81621e92-8d12-4b9a-f2a1-1c1bc01c3c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mounting to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import shutil and splitting the dataset "
      ],
      "metadata": {
        "id": "nTChDgQeVQ6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil  # provides the ability to copy and removal of files\n",
        "import os \n",
        "import random\n",
        "try:\n",
        "    shutil.rmtree(\"inaturalist_12K\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Unzip the natural datset     \n",
        "zip_path = \"drive/MyDrive/nature_12K.zip\"\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q nature_12K.zip\n",
        "\n",
        "try:\n",
        "    os.remove(\"nature_12K.zip\")\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    shutil.rmtree(\"inaturalist_12K/validation/\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# dataset directory \n",
        "temp = os.listdir(\"inaturalist_12K/train\")\n",
        "class_names = [name for name in temp if name != \".DS_Store\"] \n",
        "\n",
        "#Spliting valdiation dataset \n",
        "validation_split = 0.1 #splitting dataset 10%\n",
        "def spliting_train_data(validation_split = 0.1):\n",
        "  os.mkdir(\"inaturalist_12K/validation/\")\n",
        "  for each_class in class_names:\n",
        "      os.mkdir(\"inaturalist_12K/validation/\"+each_class)\n",
        "      train_images = os.listdir(\"inaturalist_12K/train/\"+each_class)\n",
        "      train_images_per_class = list(train_images)\n",
        "      #print(len(train_images_per_class))\n",
        "      random.shuffle(train_images_per_class) # Shuffle the training images\n",
        "\n",
        "      validation_images_per_class = train_images_per_class[:round(validation_split*len(train_images_per_class))]\n",
        "     # print(len(validation_images_per_class))\n",
        "\n",
        "      for im in validation_images_per_class:\n",
        "          shutil.move(\"inaturalist_12K/train/\"+each_class+\"/\"+im, \"inaturalist_12K/validation/\"+each_class+\"/\"+im)\n",
        "spliting_train_data(validation_split)\n"
      ],
      "metadata": {
        "id": "8-91XS9fi_e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring the Data"
      ],
      "metadata": {
        "id": "WQVhzYffXYya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = 0\n",
        "val = 0\n",
        "test = 0\n",
        "\n",
        "for each_class in class_names:\n",
        "      train_images = os.listdir(\"inaturalist_12K/train/\"+each_class)\n",
        "      train+=len(list(train_images))\n",
        "\n",
        "      validation_images = os.listdir(\"inaturalist_12K/validation/\"+each_class)\n",
        "      \n",
        "      val+=len(list(validation_images))\n",
        "\n",
        "      test_images = os.listdir(\"inaturalist_12K/val/\"+each_class)\n",
        "      \n",
        "      test+=len(list(test_images))\n",
        " # print training validation test      \n",
        "print(\"training\" , train)\n",
        "print(\"validation Examples = \", val)\n",
        "print(\"test examples = \", test)"
      ],
      "metadata": {
        "id": "kqYnlBbrAfHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c323464-a66f-4dec-b1f1-59d36c9e6c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training 9000\n",
            "validation Examples =  1000\n",
            "test examples =  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required libriaries for pre trained module "
      ],
      "metadata": {
        "id": "O4nfbgpxXqfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  \n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout, BatchNormalization\n",
        "\n",
        "\n",
        "# implement weights and biases \n",
        "\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from wandb.keras import WandbCallback\n",
        "from PIL import Image\n",
        "# image plotting \n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfaDSZlTi_hj",
        "outputId": "1c48889f-ad6c-4f4c-c8a3-be57540543f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.11-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.8-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=ddb10b4a79c22c064e9a05520ada3ea35a8e57d32f05145eb0a283fff8fb4a44\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.8 setproctitle-1.2.2 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.11 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the Train Test and Validation generators\n"
      ],
      "metadata": {
        "id": "wRmD-pv_X2-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"inaturalist_12K\"\n",
        "#data_augmentation = True\n",
        "# data prepartion \n",
        "def data_preparation(data_dir , data_augmentation , batch_size):\n",
        "  train_dir = os.path.join(data_dir, \"train\")\n",
        "  val_dir = os.path.join(data_dir, \"validation\")\n",
        "  test_dir = os.path.join(data_dir, \"val\")\n",
        "  if data_augmentation == True:\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                      height_shift_range=0.2,\n",
        "                                      width_shift_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      zoom_range=0.2,\n",
        "                                      fill_mode=\"nearest\")\n",
        "  else:\n",
        "      train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "          train_dir,\n",
        "          target_size=(224, 224),\n",
        "          shuffle=True,\n",
        "          color_mode=\"rgb\",\n",
        "          batch_size=batch_size,\n",
        "          class_mode='categorical')\n",
        "\n",
        "  val_generator = val_datagen.flow_from_directory(\n",
        "          val_dir,\n",
        "          target_size=(224, 224),\n",
        "          shuffle=True,\n",
        "          color_mode=\"rgb\",\n",
        "          batch_size=batch_size,\n",
        "          class_mode='categorical')\n",
        "  \n",
        "  test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size= (224, 224),\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "  \n",
        "  \n",
        "  return train_generator , val_generator, test_generator\n",
        "#train_generator , val_generator, test_generator = data_preparation(data_dir , data_augmentation = True , batch_size = 250)"
      ],
      "metadata": {
        "id": "P5t4ApQDi_kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement pre trained InceptionV3, InceptionResNetV2, ResNet50, Xception"
      ],
      "metadata": {
        "id": "T0nTOiOVX-u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain_model(pretrained_model_name, dropout, dense_layer, pre_layer_train=None):\n",
        "    \n",
        "  # keras sequential model based on a pre-trained model intended to be fine tuned\n",
        "  \n",
        "\n",
        "    # shape image size \n",
        "    shape = (224 ,224, 3)\n",
        "\n",
        "    # adding a pretrained model without the top dense layer\n",
        "    if pretrained_model_name == 'ResNet50':\n",
        "      pretrained_model = tf.keras.applications.ResNet50(include_top=False,input_shape=shape, weights='imagenet')\n",
        "    elif pretrained_model_name == 'InceptionV3':\n",
        "      pretrained_model = tf.keras.applications.InceptionV3(include_top=False,input_shape= shape, weights='imagenet')    \n",
        "    elif pretrained_model_name == 'Xception':\n",
        "      pretrained_model = tf.keras.applications.Xception(include_top=False,input_shape= shape, weights='imagenet')\n",
        "    elif pretrained_model_name == 'InceptionResNetV2':\n",
        "      pretrained_model = tf.keras.applications.InceptionResNetV2(include_top=False,input_shape= shape, weights='imagenet')\n",
        "    \n",
        "    else:\n",
        "      raise Exception('no pretrained model given')\n",
        "      \n",
        "\n",
        "    #freeze all layers\n",
        "    for layer in pretrained_model.layers:\n",
        "        layer.trainable=False \n",
        "    \n",
        "    #setting  top layers as trainable\n",
        "    if pre_layer_train:\n",
        "      for layer in pretrained_model.layers[-pre_layer_train:]:\n",
        "        layer.trainable=True\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    #adding pretrained model\n",
        "    model.add(pretrained_model)\n",
        "    # flatten layer  \n",
        "    model.add(Flatten()) \n",
        "    #Adding a dense layer\n",
        "    model.add(Dense(dense_layer, activation= 'relu'))\n",
        "    #Adding dropout\n",
        "    model.add(Dropout(dropout))\n",
        "    #output layer with softmax  \n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        " \n",
        "    return model"
      ],
      "metadata": {
        "id": "0HWNp_Fzi_mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration wandb\n"
      ],
      "metadata": {
        "id": "A-Nqbud5YRoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Default values for hyper-parameters\n",
        "    config_defaults = {\n",
        "        \"data_augmentation\": True,\n",
        "        \"batch_size\": 250,\n",
        "        \"dropout\": 0.1,\n",
        "        \"dense_layer\": 256,\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"epochs\": 5,\n",
        "        \"pre_layer_train\": None,\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    # Local variables, values obtained from wandb config\n",
        "    data_augmentation = config.data_augmentation\n",
        "    batch_size = config.batch_size\n",
        "    dropout = config.dropout\n",
        "    dense_layer = config.dense_layer\n",
        "    learning_rate = config.learning_rate\n",
        "    epochs = config.epochs\n",
        "    pre_layer_train = config.pre_layer_train\n",
        "\n",
        "    \n",
        "   # Display the hyperparameters\n",
        "    run_name = \"pre_train_mdl_{}_aug_{}_bs_{}_ep_{}_dropout_{}_dense_{}\".format(pre_train_model, data_augmentation, batch_size,epochs, dropout, dense_layer)\n",
        "    print(run_name)\n",
        "\n",
        "    # Create the data generators\n",
        "    train_generator , val_generator, test_generator = data_preparation(data_dir , data_augmentation = data_augmentation, batch_size = batch_size)\n",
        "    \n",
        "    # Define the model\n",
        "    model = pretrain_model(pretrained_model_name = pre_train_model, dropout = dropout, dense_layer = dense_layer, pre_layer_train=pre_layer_train)\n",
        "    print(model.count_params())\n",
        "\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate = learning_rate), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    # Early Stopping callback\n",
        "    earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "\n",
        "    # To save the model with best validation accuracy\n",
        "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "    #Training the model\n",
        "    history = model.fit(train_generator,\n",
        "                        steps_per_epoch = train_generator.n//train_generator.batch_size,\n",
        "                        validation_data = val_generator,\n",
        "                        validation_steps = val_generator.n//val_generator.batch_size,\n",
        "                        epochs=epochs, verbose = 2, \n",
        "                        callbacks=[WandbCallback(), earlyStopping, checkpoint])\n",
        "      \n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    # Meaningful name for the run\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "    wandb.run.finish()\n",
        "    return history"
      ],
      "metadata": {
        "id": "dJuaKSnFD2oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sweeps configuration for hyper parameters "
      ],
      "metadata": {
        "id": "6beHG2WCZI56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"CNN_PartB\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"val_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"method\": \"bayes\",\n",
        "  \"parameters\": {\n",
        "        \"data_augmentation\": {\n",
        "            \"values\": [True, False]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [128, 256]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001, 0.0001]\n",
        "        },\n",
        "        \"epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0, 0.2,0.4]\n",
        "        },\n",
        "        \"dense_layer\": {\n",
        "            \"values\": [128, 256, 512]\n",
        "        },\n",
        "          \"pre_layer_train\": {\n",
        "            \"values\": [None, 10, 20]\n",
        "        },\n",
        "\n",
        "      \n",
        "           }\n",
        "}\n",
        "\n",
        "\n",
        "# Generates a sweep id\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"moni6264\", project=\"test_cnn_partb\")"
      ],
      "metadata": {
        "id": "vjKhNLrmo2qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running wandb"
      ],
      "metadata": {
        "id": "mxrc4hXLKvwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note:  You might get different result because of shuffling of dataset\n",
        "pre_train_model = \"ResNet50'\"   #you can change model for wandb sweeps\n",
        "sweep_id = \"y68teyjh\"\n",
        "wandb.agent(sweep_id, train,entity=\"moni6264\", project=\"test_cnn_partb\", count=10)"
      ],
      "metadata": {
        "id": "Za5GdVu6iCEf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}